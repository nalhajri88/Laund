{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510beb9f",
   "metadata": {},
   "source": [
    "# Laundry AI Detector\n",
    "\n",
    "This notebook implements a **Laundry AI Detector**, an intelligent system designed to monitor laundry shops through video analysis. The goal is to detect situations where employees take clothes from customers without issuing a receipt within a 10‑minute window. The detector counts how many customers enter, deliver clothing, and whether or not a receipt is printed for each transaction. When a customer does not receive a receipt within the allotted time, the system flags the event and saves a short video clip for review.\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "- Video processing to extract frames at a specified rate (you can adjust the sampling rate to balance processing speed and detection accuracy).\n",
    "- Use of AI models for object detection (YOLOv8) and person tracking (DeepSORT) to follow customers and staff over time.\n",
    "- Event analysis to identify when clothing is handed over and whether a receipt is issued within 10 minutes.\n",
    "- Export functionality to create short video clips of detected events and summary logs for further investigation.\n",
    "\n",
    "This notebook is meant to serve as a starting point. You can adapt the detection logic, modify the model, or integrate your own custom dataset for receipts or specific clothing items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d22714-2453-4532-bc78-81ad4e64667c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install ultralytics deep_sort_realtime pandas tqdm opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d196dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Ultralytics settings reset to default values. This may be due to a possible problem with your settings or a recent ultralytics package update. \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/naifalhajri/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the YOLOv8 model pretrained on COCO dataset. This model detects persons and common objects.\n",
    "# You can choose 'yolov8n.pt' (nano), 'yolov8s.pt' (small), 'yolov8m.pt' (medium), etc. Adjust based on available compute.\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # using the lightweight nano version for speed\n",
    "\n",
    "# Initialize DeepSORT tracker for multi-object tracking. The parameters can be tuned for your environment.\n",
    "tracker = DeepSort(max_age=50, n_init=2, nms_max_overlap=1.0, max_cosine_distance=0.2)\n",
    "\n",
    "# Define the list of YOLO class names for reference (COCO dataset)\n",
    "CLASS_NAMES = model.model.names\n",
    "\n",
    "# Define which class IDs represent clothing or bag items. This is heuristic and can be adjusted.\n",
    "CLOTHING_CLASSES = {\n",
    "    CLASS_NAMES.index('handbag'),\n",
    "    CLASS_NAMES.index('backpack'),\n",
    "    CLASS_NAMES.index('suitcase'),\n",
    "    CLASS_NAMES.index('tie'),\n",
    "    CLASS_NAMES.index('teddy bear'),  # use for soft items if needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_receipt_region(frame, debug=False):\n",
    "    \"\"\"\n",
    "    Detects potential receipt regions in the frame based on simple colour and shape heuristics.\n",
    "    A receipt is usually a small, bright (mostly white) rectangular piece of paper. This function\n",
    "    applies thresholding to isolate bright regions and then filters by aspect ratio and size.\n",
    "\n",
    "    Parameters:\n",
    "        frame (np.ndarray): BGR image frame from video.\n",
    "        debug (bool): If True, displays intermediate images for tuning (requires interactive environment).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a receipt-like region is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Focus on bright areas; adjust threshold as needed\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "    # Morphological operations to remove noise and close gaps\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        if area < 500 or area > 50000:\n",
    "            continue  # discard too small or too large regions\n",
    "        aspect_ratio = w / float(h)\n",
    "        # Typical receipt is roughly tall and narrow or long and thin; adjust thresholds as needed\n",
    "        if 0.3 < aspect_ratio < 3.0:\n",
    "            # This region may be a receipt\n",
    "            if debug:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.imshow('Receipt candidate', frame)\n",
    "                cv2.waitKey(0)\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video(\n",
    "    video_path: str,\n",
    "    output_dir: str,\n",
    "    receipt_timeout: int = 600,\n",
    "    frame_sample_rate: int = 3,\n",
    "    debug: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a video file to detect clothing drop-off events and checks for receipt issuance.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to input MP4 video file.\n",
    "        output_dir (str): Directory where event clips and logs will be saved.\n",
    "        receipt_timeout (int): Time window (in seconds) to wait for a receipt after clothing is handed over.\n",
    "        frame_sample_rate (int): Process every Nth frame for efficiency.\n",
    "        debug (bool): If True, shows intermediate frames and debugging information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame summarising detected events.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_basename = os.path.basename(video_path).rsplit('.', 1)[0]\n",
    "\n",
    "    # Dictionaries to track state per person ID\n",
    "    person_last_seen = {}\n",
    "    clothing_event_time = {}\n",
    "    receipt_received = {}\n",
    "\n",
    "    # List to collect event logs\n",
    "    event_logs = []\n",
    "\n",
    "    frame_idx = 0\n",
    "    pbar = tqdm(total=total_frames, desc='Processing video')\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Skip frames to speed up processing\n",
    "        if frame_idx % frame_sample_rate != 0:\n",
    "            continue\n",
    "\n",
    "        timestamp = frame_idx / fps\n",
    "\n",
    "        # Run YOLO detection\n",
    "        results = model(frame, verbose=False)\n",
    "        detections = []\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            if conf < 0.4:\n",
    "                continue  # filter low confidence\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            # Detections formatted for DeepSort: [x1, y1, x2, y2, confidence, class_id]\n",
    "            detections.append([x1, y1, x2, y2, conf, cls_id])\n",
    "\n",
    "        # Update tracker\n",
    "        tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        # For each tracked person, update states\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            l, t, r, b = track.to_ltrb()\n",
    "            class_id = track.det_class\n",
    "\n",
    "            # Only care about person class\n",
    "            if class_id != CLASS_NAMES.index('person'):\n",
    "                continue\n",
    "\n",
    "            # Update last seen time\n",
    "            person_last_seen[track_id] = timestamp\n",
    "\n",
    "            # Check if person is carrying clothes based on detection of a clothing-like object nearby\n",
    "            carrying_clothes = False\n",
    "            for det in detections:\n",
    "                dx1, dy1, dx2, dy2, dconf, dcls = det\n",
    "                if int(dcls) in CLOTHING_CLASSES:\n",
    "                    # Compute IoU with person box\n",
    "                    inter_x1 = max(l, dx1)\n",
    "                    inter_y1 = max(t, dy1)\n",
    "                    inter_x2 = min(r, dx2)\n",
    "                    inter_y2 = min(b, dy2)\n",
    "                    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "                    person_area = (r - l) * (b - t)\n",
    "                    if person_area > 0 and (inter_area / person_area) > 0.1:\n",
    "                        carrying_clothes = True\n",
    "                        break\n",
    "\n",
    "            # Record time of clothing drop-off\n",
    "            if carrying_clothes and track_id not in clothing_event_time:\n",
    "                clothing_event_time[track_id] = timestamp\n",
    "                receipt_received[track_id] = False\n",
    "                if debug:\n",
    "                    print(f\"Person {track_id} dropped off clothes at {timestamp:.2f}s\")\n",
    "\n",
    "            # Detect receipt issuance after drop-off\n",
    "            if track_id in clothing_event_time and not receipt_received[track_id]:\n",
    "                if detect_receipt_region(frame):\n",
    "                    receipt_received[track_id] = True\n",
    "                    if debug:\n",
    "                        print(f\"Receipt issued to person {track_id} at {timestamp:.2f}s\")\n",
    "\n",
    "        # Check for timeouts\n",
    "        to_remove = []\n",
    "        for pid, drop_time in clothing_event_time.items():\n",
    "            if receipt_received[pid]:\n",
    "                to_remove.append(pid)\n",
    "                continue\n",
    "            # If enough time has passed without receipt, flag event\n",
    "            if timestamp - drop_time > receipt_timeout:\n",
    "                # Save video clip around the event (10 seconds before and after drop)\n",
    "                clip_start = max(0, drop_time - 5)\n",
    "                clip_end = min(timestamp + 5, total_frames / fps)\n",
    "                clip_filename = f\"{video_basename}_person{pid}_no_receipt_{int(drop_time)}.mp4\"\n",
    "                clip_path = os.path.join(output_dir, clip_filename)\n",
    "                save_clip(video_path, clip_start, clip_end, clip_path)\n",
    "                event_logs.append({\n",
    "                    'person_id': pid,\n",
    "                    'drop_time_sec': drop_time,\n",
    "                    'clip_path': clip_path,\n",
    "                    'receipt_received': False\n",
    "                })\n",
    "                to_remove.append(pid)\n",
    "                if debug:\n",
    "                    print(f\"No receipt for person {pid} (drop at {drop_time:.2f}s)\")\n",
    "\n",
    "        # Clean up processed IDs\n",
    "        for pid in to_remove:\n",
    "            clothing_event_time.pop(pid, None)\n",
    "            receipt_received.pop(pid, None)\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "\n",
    "    # Save event logs to CSV\n",
    "    df_events = pd.DataFrame(event_logs)\n",
    "    csv_path = os.path.join(output_dir, f\"{video_basename}_event_log.csv\")\n",
    "    df_events.to_csv(csv_path, index=False)\n",
    "    print(f\"Processed {video_path}. Events logged to {csv_path}.\")\n",
    "    return df_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_clip(video_path: str, start_time: float, end_time: float, output_path: str):\n",
    "    \"\"\"\n",
    "    Saves a segment of the input video between start_time and end_time to output_path.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the original video.\n",
    "        start_time (float): Start time in seconds.\n",
    "        end_time (float): End time in seconds.\n",
    "        output_path (str): Path to save the extracted clip.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Compute frame ranges\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "\n",
    "    current_frame = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if current_frame >= start_frame and current_frame <= end_frame:\n",
    "            out.write(frame)\n",
    "        current_frame += 1\n",
    "        if current_frame > end_frame:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc26b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "# Provide the path to your input MP4 video and an output directory to store results.\n",
    "# Adjust receipt_timeout (seconds) as needed. Here we set it to 600s (10 minutes).\n",
    "\n",
    "# Uncomment and set your paths accordingly\n",
    "# video_file = 'path/to/your/laundry_shop_video.mp4'\n",
    "# output_directory = 'output_events'\n",
    "# events_df = process_video(\n",
    "#     video_path=video_file,\n",
    "#     output_dir=output_directory,\n",
    "#     receipt_timeout=600,\n",
    "#     frame_sample_rate=3,\n",
    "#     debug=True\n",
    "# )\n",
    "# events_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
